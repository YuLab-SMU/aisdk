% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/agent_evals.R
\name{expect_llm_pass}
\alias{expect_llm_pass}
\title{Expect LLM Pass}
\usage{
expect_llm_pass(response, criteria, model = NULL, threshold = 0.7, info = NULL)
}
\arguments{
\item{response}{The LLM response to evaluate (text or GenerateResult object).}

\item{criteria}{Character string describing what constitutes a passing response.}

\item{model}{Model to use for judging (default: same as response or gpt-4o).}

\item{threshold}{Minimum score (0-1) to pass (default: 0.7).}

\item{info}{Additional information to include in failure message.}
}
\value{
Invisibly returns the evaluation result.
}
\description{
Custom testthat expectation that evaluates whether an LLM response
meets specified criteria. Uses an LLM judge to assess the response.
}
\examples{
\dontrun{
test_that("agent answers math questions correctly", {
  result <- generate_text(
    model = "openai:gpt-4o",
    prompt = "What is 2 + 2?"
  )
  expect_llm_pass(result, "The response should contain the number 4")
})
}
}
